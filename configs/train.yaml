defaults:
    - _self_
    - dataset: default
    - optim: adamw
    - model: mm_cross_attn_model
    - loss_fn: msle  # Use standard MSLE loss for simplicity

# Override default dataset config
dataset:
  batch_size: 64  # Recommended for ViT in the report
  num_workers: 16
  metadata: ["title", "description"]  # Use both title and description 
  train_transform: advanced  # Use our enhanced transforms

# Training parameters
epochs: 30
log: True
prefix: "MMCA_MSLE_"
experiment_name: ${prefix}${model.name}_${now:%Y-%m-%d_%H-%M-%S}

# Learning rate scheduler
lr_scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 5e-4
  total_steps: null  # Will be calculated in train.py
  pct_start: 0.3
  div_factor: 25
  final_div_factor: 1000

# Override optimizer settings
optim:
  _target_: torch.optim.AdamW
  lr: 2e-5  # Starting learning rate (will be managed by scheduler)
  betas: [0.9, 0.999]
  weight_decay: 0.01  # L2 regularization

hydra:
  output_subdir: null
  run:
    dir: .

datamodule:
  _target_: data.datamodule.DataModule
  dataset_path: ${data_dir}
  train_transform: ${dataset.train_transform}
  test_transform: ${dataset.val_transform}
  batch_size: ${dataset.batch_size}
  num_workers: ${dataset.num_workers}
  metadata: ${dataset.metadata}

data_dir: ${root_dir}/dataset/
root_dir: ${hydra:runtime.cwd}
checkpoint_path: ${root_dir}/checkpoints/${experiment_name}.pt

resume_from_checkpoint: False