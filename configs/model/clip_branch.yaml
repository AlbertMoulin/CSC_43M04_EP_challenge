instance:
  _target_: models.enhanced_clip_branch.EnhancedCLIPBranchModel
  clip_model_name: "openai/clip-vit-base-patch32"
  embed_dim: 512                    # Dimension after modality-specific reduction
  num_heads: 8                      # Number of attention heads
  num_channels: 1000                # Will be overridden with actual num_channels from dataset
  freeze_clip: True                 # Freeze CLIP backbone
  dropout_rate: 0.1                 # Dropout rate throughout the model
  metadata_embed_dim: 128           # Dimension for metadata features
  final_mlp_layers: [1024, 1024, 1024, 1]  # Final MLP architecture

name: EnhancedCLIPBranchModel