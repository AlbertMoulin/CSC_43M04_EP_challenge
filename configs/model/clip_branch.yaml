instance:
  _target_: models.clip_branch.CLIPBranchModel
  clip_mlp_layers: [1024, 1024, 1024] # Removed the final '1'
  date_mlp_layers: [64, 32, 16]      # Removed the final '1'
  channel_mlp_layers: [128, 64, 32]  # Removed the final '1'
  clip_model_name: "openai/clip-vit-base-patch32"
  num_channels: 20000 
  freeze_clip: True
  dropout_rate: 0.2

name: CLIPBranchModel